# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# environment/computational parameters
seed: 32678456782       # random number generator seed (long integer value)
device: cuda            # if you have multiple GPU's, you can use 'cude:4' to specify which GPU to run on, e.g., the 4th
num_workers: 4          # number of CPU cores that load data in parallel. You can set this to the number of logical CPU cores that you have. 

# dataset parameters
data_root: '/home/Mariam/codes/ct_classifier/data/split_temporal'
num_classes: 2
normalization_factor: 10000 #divide all channels by this value to normalize

# training hyperparameters
image_size: [120, 120]
num_epochs: 200         # number of epochs. Each epoch has multiple iterations. In each epoch the model goes over the full dataset once.
batch_size: 128         # number of images that are processed in parallel in every iteration
learning_rate: 0.001    # hyperparameter to adjust the optimizer's learning rate 
weight_decay: 0.001     # hyperparameter for regularization

# resnet depth
layers: 50              # number of ResNet layers